checkpointLocation = "file:///home/stefano/Documents/SPARKINGEST/checkpoints"

sources {

  //example of configuration for csv files
  file {
    required {
      path = "/home/stefano/Documents/SPARKINGEST/*.csv"
      format = "csv"
    }
    optional {
      refreshTime = 1 //check every X ms for new files
      sep = ","
    }
    //IF SCHEMA IS PRESENT, IT WILL BE TAKEN
    schema = [
      {name: string},
      {surname: string}
      {age: int}
    ]
  }

  kafka {
    kafka-bootstrap-servers = "localhost:9092" //comma separated list

    required {
      subscribe = "ingestion" //topic(s) to subscribe to
    }
    optional {
      startingOffsets = "earliest" //earliest: from start of kafka topic, latest: only new data from now.
    }
  }

  es {
    index = "spark"
    logType = "people"

    required {}
    optional {
      checkpointLocation = "/home/stefano/Documents/SPARKINGESTES"
    }
  }

}

sinks {

  file {
    required {
      path = "file:///home/stefano/Documents/SPARKINGEST"
      format = "json"
    }
    optional {
      outputMode = "append"
    }
  }

  //writes whole row to kafka topic as JSON
  kafka {
    kafka-bootstrap-servers = "localhost:9092"

    required {
      topic = "enriched"
    }
    optional {}
  }

  console {
    required {}
    optional {}
  }

  es {
    index = "spark"
    type = "people"

    required {}
    optional {}
  }
}


//transformation are all optionals, and will be executed sequentially
transformations {
  date {
    column = "sparkProcessTime"
  }

  watermark {
    column = "sparkProcessTime"
    duration = "10 minute"
  }

  add {
    columns {

      dummyNum1 {
        value = "1"
      }


      dummyNum2 {
        value = "2"
      }

    }
  }

  remove {
    columns = ["dummyNum2"]
  }
}