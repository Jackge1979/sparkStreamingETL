application {
  checkpointLocation = "file:///home/stefano/Documents/SPARKINGEST/checkpoints"
}

sources {

  sourceToApply = "es"

  //example of configuration for csv files
  file {
    required {
      path = "/home/stefano/Documents/SPARKINGEST/*.csv"
      format = "csv"
    }
    optional {
      refreshTime = 1 //check every X ms for new files
      sep = ","
    }
    //IF SCHEMA IS PRESENT, IT WILL BE TAKEN
    schema = [
      {name: string},
      {surname: string}
      {age: int}
    ]
  }

  kafka {
    kafka-bootstrap-servers = "localhost:9092" //comma separated list

    required {
      subscribe = "ingestion" //topic(s) to subscribe to
    }
    optional {
      startingOffsets = "earliest" //earliest: from start of kafka topic, latest: only new data from now.
    }
  }

  static {
    //WARNING: THIS IS A STATIC SOURCE. Adding it because in the future could be nice to join to (e.g. join stream with static es)
    es {
      index = "spark"
      type = "people"

      columns="sparkProcessTime,age"
      filter="age < 0"

      required {
        path = ${sources.static.es.index}/${sources.static.es.type}
        nodes = "localhost:9200"
      }
      optional {}
    }

  }

}

sinks {

  sinkToApply = "console"

  file {
    required {
      path = "file:///home/stefano/Documents/SPARKINGEST"
      format = "json"
    }
    optional {
      outputMode = "append"
    }
  }

  //writes whole row to kafka topic as JSON
  kafka {
    kafka-bootstrap-servers = "localhost:9092"

    required {
      topic = "enriched"
    }
    optional {}
  }

  console {
    required {}
    optional {
      outputMode = "update"
    }
  }

  es {
    index = "spark"
    type = "people"

    required {}
    optional {}
  }
}

transformations {
  //defined order of transformations to apply
  order = ["date", "watermark", "add"]

  date {
    column = "sparkProcessTime"
  }

  watermark {
    column = "sparkProcessTime"
    duration = "10 minute"
  }

  add {
    columns {

      dummyNum1 {
        value = "1"
      }


      dummyNum2 {
        value = "2"
      }

    }
  }

  filter = [
    "name LIKE 'stefano%'"
  ]
  select = [
    "sparkProcessTime", "name"
  ]

  remove {
    columns = []
  }

  count {
    watermarkColumn = "sparkProcessTime"
    windowTime = "2 minute"
    slidingTime = "1 minute"
    columns = ["dummyNum1"]
  }

}

listeners {

  listenersToApply = ["file"]

  kafka {
    topic = "metrics"
    servers = ${sources.kafka.kafka-bootstrap-servers}
    keySerializer = "org.apache.kafka.common.serialization.StringSerializer"
    valueSerializer = "org.apache.kafka.common.serialization.StringSerializer"
  }

  file {
    filename = "/home/stefano/Documents/SPARKINGEST/sparkstream.log"
  }

}
